{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTNU1mwGB1ZD"
      },
      "source": [
        "**Dependencies and setup**\n",
        "\n",
        "This can take a minute or so..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rA38jtUgtZsG",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# the FORK implementation was based on https://github.com/honghaow/FORK\n",
        "# some parts of the model implementation have been taken from https://github.com/Rafael1s/Deep-Reinforcement-Learning-Algorithms/tree/master/BipedalWalkerHardcore-TD3-FORK\n",
        "\n",
        "%%capture\n",
        "!apt update\n",
        "!pip install 'gym[box2d]'\n",
        "!apt install xvfb -y\n",
        "!pip install pyvirtualdisplay\n",
        "!pip install utils\n",
        "\n",
        "import gym\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "import copy\n",
        "import utils\n",
        "from gym.wrappers.record_video import RecordVideo\n",
        "from pyvirtualdisplay import Display\n",
        "from IPython import display as disp\n",
        "from collections import deque, namedtuple\n",
        "%matplotlib inline\n",
        "\n",
        "display = Display(visible=0,size=(600,600))\n",
        "display.start()\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "\n",
        "plot_interval = 10 # update the plot every N episodes\n",
        "video_every = 25 # videos can take a very long time to render so only do it every N episodes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJHtclV_30Re"
      },
      "source": [
        "**Reinforcement learning agent**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "O9Y_qHhuQq7W",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer(object):\n",
        "  def __init__(self, state_dim, action_dim, max_size=int(1e6)):\n",
        "    self.max_size = max_size\n",
        "    self.ptr = 0\n",
        "    self.size = 0\n",
        "\n",
        "    self.state = np.zeros((max_size, state_dim))\n",
        "    self.action = np.zeros((max_size, action_dim))\n",
        "    self.next_state = np.zeros((max_size, state_dim))\n",
        "    self.reward = np.zeros((max_size, 1))\n",
        "    self.done = np.zeros((max_size, 1))\n",
        "  \n",
        "  def push(self, state, action, next_state, reward, done):\n",
        "    self.state[self.ptr] = state\n",
        "    self.action[self.ptr] = action\n",
        "    self.next_state[self.ptr] = next_state\n",
        "    self.reward[self.ptr] = reward\n",
        "    self.done[self.ptr] = done\n",
        "\n",
        "    self.ptr = (self.ptr + 1) % self.max_size\n",
        "    self.size = min(self.size + 1, self.max_size)\n",
        "\n",
        "  def sample(self, batch_size):\n",
        "    ind = np.random.randint(0,int(self.size), size=batch_size)\n",
        "    return (\n",
        "      torch.FloatTensor(self.state[ind]).to(device),\n",
        "      torch.FloatTensor(self.action[ind]).to(device),\n",
        "      torch.FloatTensor(self.next_state[ind]).to(device),\n",
        "      torch.FloatTensor(self.reward[ind]).to(device),\n",
        "      torch.FloatTensor(self.done[ind]).to(device)\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "4jXNHP8_U-rn",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "class Actor(nn.Module):\n",
        "\n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    super(Actor, self).__init__()\n",
        "    self.fc1 = nn.Linear(state_dim, 256)\n",
        "    self.fc2 = nn.Linear(256, 256)\n",
        "    self.fc3 = nn.Linear(256, action_dim)\n",
        "    self.max_action = max_action\n",
        "\n",
        "  def forward(self, state):\n",
        "    x = F.relu(self.fc1(state))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = torch.tanh(self.fc3(x))\n",
        "    return self.max_action * x\n",
        "\n",
        "\n",
        "class Critic(nn.Module):\n",
        "\n",
        "  def __init__(self, state_dim, action_dim):\n",
        "    super(Critic, self).__init__()\n",
        "    self.l1 = nn.Linear(state_dim + action_dim, 256)\n",
        "    self.l2 = nn.Linear(256, 256)\n",
        "    self.l3 = nn.Linear(256, 1)\n",
        "    self.l4 = nn.Linear(state_dim + action_dim, 256)\n",
        "    self.l5 = nn.Linear(256, 256)\n",
        "    self.l6 = nn.Linear(256, 1)\n",
        "\n",
        "  def forward(self, state, action):\n",
        "    x = torch.cat([state, action], 1)\n",
        "    q1 = F.relu(self.l1(x))\n",
        "    q1 = F.relu(self.l2(q1))\n",
        "    q2 = F.relu(self.l4(x))\n",
        "    q2 = F.relu(self.l5(q2))\n",
        "    return self.l3(q1), self.l6(q2)\n",
        "\n",
        "\n",
        "class Sys(nn.Module):\n",
        "\n",
        "  def __init__(self, state_dim, action_dim):\n",
        "    super(Sys, self).__init__()\n",
        "    self.l1 = nn.Linear(state_dim + action_dim, 400)\n",
        "    self.l2 = nn.Linear(400, 300)\n",
        "    self.l3 = nn.Linear(300, state_dim)\n",
        "\n",
        "  def forward(self, state, action):\n",
        "    x = torch.cat([state, action], 1)\n",
        "    predict = F.relu(self.l1(x))\n",
        "    predict = F.relu(self.l2(predict))\n",
        "    return self.l3(predict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "kHx0mu0jwz5c",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "class TD3(object):\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      env,\n",
        "      state_dim,\n",
        "      action_dim,\n",
        "      max_action,\n",
        "      policy_noise = 0.1,\n",
        "      noise_clip = 0.5,\n",
        "      policy_freq = 2,\n",
        "      sys_weight = 0.5,\n",
        "      sys_threshold = 0.02,\n",
        "      tau = 0.005,\n",
        "      lr = 0.001,\n",
        "      gamma = 0.99\n",
        "      ):\n",
        "\n",
        "    self.env = env\n",
        "\n",
        "    self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
        "    self.actor_target = copy.deepcopy(self.actor)\n",
        "    self.actor_optimiser = torch.optim.Adam(self.actor.parameters(), lr=lr)\n",
        "\n",
        "    self.critic = Critic(state_dim, action_dim).to(device)\n",
        "    self.critic_target = copy.deepcopy(self.critic)\n",
        "    self.critic_optimiser = torch.optim.Adam(self.critic.parameters(), lr=lr)\n",
        "\n",
        "    self.sys = Sys(state_dim, action_dim).to(device)\n",
        "    self.sys_optimiser = torch.optim.Adam(self.sys.parameters(), lr=lr)\n",
        "    self.sys.apply(self.weights)\n",
        "    self.sys_loss = 0\n",
        "\n",
        "    self.upper = float(self.env.action_space.high[0])\n",
        "    self.lower = float(self.env.action_space.low[0])\n",
        "    self.obs_upper = float(self.env.observation_space.high[0])\n",
        "    self.obs_lower = float(self.env.observation_space.low[0])\n",
        "\n",
        "    self.max_action = max_action\n",
        "    self.policy_noise = policy_noise\n",
        "    self.noise_clip = noise_clip\n",
        "    self.policy_freq = policy_freq\n",
        "    self.sys_weight = sys_weight\n",
        "    self.sys_threshold = sys_threshold\n",
        "    self.tau = tau\n",
        "    self.gamma = gamma\n",
        "\n",
        "  def weights(self, layer):\n",
        "    if type(layer) == nn.Linear:\n",
        "      torch.nn.init.xavier_normal_(layer.weight)\n",
        "      layer.bias.data.fill_(0.001)\n",
        "  \n",
        "  def select_action(self, state):\n",
        "    state = torch.FloatTensor(state.reshape(1, -1)).to(device)\n",
        "    return self.actor(state).cpu().data.numpy().flatten()\n",
        "  \n",
        "  def train(self, replay_buffer, batch_size, train_steps):\n",
        "    for i in range(train_steps):\n",
        "\n",
        "      state, action, next_state, reward, done = replay_buffer.sample(batch_size)\n",
        "\n",
        "      with torch.no_grad():\n",
        "\n",
        "        noise = (\n",
        "            torch.randn_like(action) * self.policy_noise\n",
        "        ).clamp(-self.noise_clip, self.noise_clip)\n",
        "        next_action = (\n",
        "            self.actor_target(next_state) + noise\n",
        "        ).clamp(-self.max_action, self.max_action)\n",
        "        target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
        "        target_Q = torch.min(target_Q1, target_Q2)\n",
        "        target_Q = reward + (1 - done) * self.gamma * target_Q\n",
        "\n",
        "      current_Q1, current_Q2 = self.critic(state, action)\n",
        "      loss_Q1 = F.mse_loss(current_Q1, target_Q)\n",
        "      loss_Q2 = F.mse_loss(current_Q2, target_Q)\n",
        "      critic_loss = loss_Q1 + loss_Q2\n",
        "      self.critic_optimiser.zero_grad()\n",
        "      critic_loss.backward()\n",
        "      self.critic_optimiser.step()\n",
        "\n",
        "      predict_next_state = self.sys(state, action).clamp(self.obs_lower, self.obs_upper)\n",
        "      sys_loss = F.smooth_l1_loss(predict_next_state, next_state.detach())\n",
        "\n",
        "      self.sys_optimiser.zero_grad()\n",
        "      sys_loss.backward()\n",
        "      self.sys_optimiser.step()\n",
        "      self.sys_loss = sys_loss.item()\n",
        "\n",
        "      s_flag = 1 if sys_loss.item() < self.sys_threshold else 0\n",
        "\n",
        "      if i % self.policy_freq == 0:\n",
        "        actor_loss_1,_ = self.critic_target(state, self.actor(state))\n",
        "        actor_loss_1 = actor_loss_1.mean()\n",
        "        actor_loss_1 = - actor_loss_1\n",
        "        if s_flag == 1:\n",
        "          p_next = self.sys(state, self.actor(state)).clamp(self.obs_lower, self.obs_upper)\n",
        "          p_actions = self.actor(p_next.detach()) * self.upper\n",
        "          actor_loss_2,_ = self.critic_target(p_next.detach(), p_actions)\n",
        "          actor_loss_2 = actor_loss_2.mean()\n",
        "          p_next_2 = self.sys(p_next.detach(), p_actions).clamp(self.obs_lower, self.obs_upper)\n",
        "          p_actions_2 = self.actor(p_next_2.detach()) * self.upper\n",
        "          actor_loss_3,_ = self.critic_target(p_next_2.detach(), p_actions_2)\n",
        "          actor_loss_3 = actor_loss_3.mean()\n",
        "          actor_loss = actor_loss_1 - (self.sys_weight * actor_loss_2) - (0.5 * self.sys_weight * actor_loss_3)\n",
        "        else:\n",
        "          actor_loss = actor_loss_1\n",
        "      \n",
        "        self.critic_optimiser.zero_grad()\n",
        "        self.sys_optimiser.zero_grad()\n",
        "\n",
        "        self.actor_optimiser.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        self.actor_optimiser.step()\n",
        "\n",
        "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "          target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
        "        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "          target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEv4ZjXmyrHo"
      },
      "source": [
        "**Prepare the environment and wrap it to capture videos**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "1Xrcek4hxDXl",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "env = gym.make(\"BipedalWalker-v3\")\n",
        "# env = gym.make(\"Pendulum-v0\") # useful continuous environment for quick experiments\n",
        "# env = gym.make(\"BipedalWalkerHardcore-v3\") # a more advanced environment\n",
        "env = RecordVideo(env, \"./video\", episode_trigger=lambda ep_id: ep_id%video_every == 0)\n",
        "\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "max_action = float(env.action_space.high[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FUw4h980jfnu",
        "outputId": "231e5b4f-d443-4ff0-f028-9b8c77b8d14d",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The environment has 24 observations and the agent can take 4 actions\n",
            "The device is: cpu\n"
          ]
        }
      ],
      "source": [
        "print('The environment has {} observations and the agent can take {} actions'.format(state_dim, action_dim))\n",
        "print('The device is: {}'.format(device))\n",
        "\n",
        "\n",
        "if device.type != 'cpu': print('It\\'s recommended to train on the cpu for this')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rDl6ViIDlVOk",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "env.reset(seed=seed)\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "env.action_space.seed(seed)\n",
        "\n",
        "# logging variables\n",
        "batch_size = 100\n",
        "start_t = 0 # set to 10000 when testing the model on the hardcore environment\n",
        "expl_noise = 0.1\n",
        "total_t = 0\n",
        "ep_reward = 0\n",
        "avg_reward = 0\n",
        "reward_list = []\n",
        "plot_data = []\n",
        "log_f = open(\"agent-log.txt\",\"w+\")\n",
        "\n",
        "# initialise agent\n",
        "agent = TD3(env, state_dim, action_dim, max_action)\n",
        "replay_buffer = ReplayBuffer(state_dim, action_dim)\n",
        "max_episodes = 500\n",
        "max_timesteps = 2000\n",
        "\n",
        "for episode in range(1, max_episodes + 1):\n",
        "  state = env.reset()\n",
        "  done = False\n",
        "  for t in range(1, max_timesteps + 1):\n",
        "    total_t += 1\n",
        "    if total_t < start_t:\n",
        "      action = env.action_space.sample()\n",
        "    else:\n",
        "      action = (\n",
        "          agent.select_action(np.array(state))\n",
        "          + np.random.normal(0, max_action * expl_noise, size = action_dim)\n",
        "      ).clip(-max_action, max_action)\n",
        "    \n",
        "    next_state, reward, done, _ = env.step(action)\n",
        "    replay_buffer.push(state, action, next_state, reward, done)\n",
        "    state = next_state\n",
        "\n",
        "    ep_reward += reward\n",
        "    avg_reward += reward\n",
        "\n",
        "    if (done or t >= max_timesteps):\n",
        "      agent.update_sys = 0\n",
        "      if total_t >= start_t:\n",
        "        agent.train(replay_buffer, batch_size, t)\n",
        "      reward_list.append(avg_reward)\n",
        "      break\n",
        "\n",
        "    total_t += 1\n",
        "\n",
        "  log_f.write('episode: {}, reward: {}\\n'.format(episode, ep_reward))\n",
        "  log_f.flush()\n",
        "  ep_reward = 0\n",
        "  avg_reward = 0\n",
        "\n",
        "  if episode % plot_interval == 0:\n",
        "    plot_data.append([episode, np.array(reward_list).mean(), np.array(reward_list).std()])\n",
        "    reward_list = []\n",
        "    # plt.rcParams['figure.dpi'] = 100\n",
        "    plt.plot([x[0] for x in plot_data], [x[1] for x in plot_data], '-', color='tab:grey')\n",
        "    plt.fill_between([x[0] for x in plot_data], [x[1]-x[2] for x in plot_data], [x[1]+x[2] for x in plot_data], alpha=0.2, color='tab:grey')\n",
        "    plt.xlabel('Episode number')\n",
        "    plt.ylabel('Episode reward')\n",
        "    plt.show()\n",
        "    disp.clear_output(wait=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "part2-agent-code",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
